{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANkd7u1TK92b"
      },
      "source": [
        "# 安裝需要的套件\n",
        "* langchain：基本的langchain套件\n",
        "* openai：基本的openai套件\n",
        "* unstructured：讀取文字檔格式的套件\n",
        "* chromadb：向量儲存資料庫\n",
        "* tiktoken套件：OpenAI算token數的套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e9dWmxdK92c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "# !pip install openai\n",
        "!pip install langchain-openai\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdedMiSsK92d"
      },
      "source": [
        "## 將環境變數讀入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxT0u_yYK92d"
      },
      "outputs": [],
      "source": [
        "# 導入 ColabSecrets 用戶資料模組\n",
        "from google.colab import userdata\n",
        "\n",
        "# 設置 OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU3QjAFZK92d"
      },
      "source": [
        "### 先套用OpenAI的API\n",
        "使用`langchain`中的`OpenAI`套件載入大型語言模型，載入OpenAi模型，並且設定最大輸出長度為1024。此部分會收費"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_LvzHrnK92e"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=512,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 測試沒有RAG時候的問答"
      ],
      "metadata": {
        "id": "Kryz-tpi3ZTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"工專時期第3任校長是誰?\")"
      ],
      "metadata": {
        "id": "k6nBEb9qsgdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"明新科技大學的校訓是什麼?\")"
      ],
      "metadata": {
        "id": "FEfGohnT3YAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_NDQiasK92f"
      },
      "source": [
        "### 建立本機知識庫QA機器人\n",
        "[Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2KsjlHTK92f"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI,VectorDBQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# 載入資料夾中所有TXT檔案\n",
        "loader = DirectoryLoader('/content/', glob='**/*.txt')\n",
        "\n",
        "# 將資料轉成document物佚，每個檔案會為作為一個document\n",
        "documents = loader.load()\n",
        "\n",
        "# 初始化載入器\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "\n",
        "# 切割加载的 document\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 初始化 openai 的 embeddings 物件\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 將 document 透過 openai 的 embeddings 物件計算 embedding向量資料暫時存入 Chroma 向量資料庫用於後續的搜尋\n",
        "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
        "\n",
        "# 建立回答物件\n",
        "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
        "\n",
        "# 進行回答\n",
        "result = qa({\"query\": \"工專時期第3任校長是誰?\"})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH35TKRmK92g"
      },
      "outputs": [],
      "source": [
        "result = qa({\"query\": \"現行明新科技大學之校訓?\"})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文件分割器的chunk_overlap參數，切分後每個文件裡包含幾個上一個文件結尾的內容，主要作用是為了增加每個文件的上下文關聯。比如chunk_overlap=0時，第一個文件為aaaaaa，第二個為bbbbbb；當chunk_overlap=2時，第一個文件為aaaaaa，第二個為aabbbbbb。"
      ],
      "metadata": {
        "id": "CMESifqj22KL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiu3ZiqK92g"
      },
      "source": [
        "## 替模型加入記憶功能\n",
        "「對話記憶體」（ConversationBufferMemory）用於儲存簡單的對話歷史 \\\n",
        "[ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer/) \\\n",
        "[memory_management](https://python.langchain.com/docs/use_cases/chatbots/memory_management/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbWrd1YoK92g"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 建立記憶體實例，開啟 return_messages 是為了將記憶體指定給 chat模型\n",
        "# 而 memory_key則是可以讓我們客制我們取得對話記錄時用的 key 值\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 建立 chat 語言模型\n",
        "# llm_chat = ChatOpenAI()\n",
        "\n",
        "# 提示設計\n",
        "prompt_chat = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"你是一個友善的學習助理，你接下來會跟使用者來對話。\"\n",
        "        ),\n",
        "        # 這裏是一個讓記憶體資料填空的地方。\n",
        "        # 我們也要設定，使用chat_history 來取得對話記錄\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "conversation_chat = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_chat,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbzh-3woK92g"
      },
      "outputs": [],
      "source": [
        "conversation_chat({\n",
        "    'question': '你好'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srMS_TyXK92g"
      },
      "outputs": [],
      "source": [
        "conversation_chat({\n",
        "    'question': '你可以告訴我英國和美國的首都在哪裡嗎?'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#查詢記憶內容\n",
        "print(\"chat_history:\", memory.load_memory_variables({}))"
      ],
      "metadata": {
        "id": "a8Y1dcHLxvFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZE-HyHAK92h"
      },
      "source": [
        "## 進階記憶功能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUyJONY6K92h"
      },
      "source": [
        "##### ConversationBufferWindowMemory 類別\n",
        "直譯為「局部窗口對話記憶體」。它的主要功能是限制在一個局部窗口內保存的對話資訊。由於 token 的運算資源有限且需消耗費用，甚至如果語言模型是我們自己架設的，同樣需要大量的運算資源，因此我們不能讓歷史對話資料無窮無盡地累積。\n",
        "\n",
        "使用ConversationBufferWindowMemory 類別，可以只保存最近的 k 條訊息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG2rlNm5K92h"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# 建立 ConversationBufferWindowMemory 實例, k=1 即限制一條訊息\n",
        "memory_buffer_window = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "# 更新上下文資訊\n",
        "memory_buffer_window.save_context({\"input\": \"你好！\"}, {\"output\": \"什麼事？\"})\n",
        "memory_buffer_window.save_context({\"input\": \"今天天氣真好！\"}, {\"output\": \"我覺得太熱了！\"})\n",
        "memory_buffer_window.save_context({\"input\": \"這是最新的訊息\"}, {\"output\": \"只會記錄這個訊息！\"})\n",
        "# 取得記憶體內儲存的資訊\n",
        "memory_buffer_window.load_memory_variables({})\n",
        "\n",
        "#--- 實際的輸出 ---\n",
        "\n",
        "# {'history': 'Human: 這是最新的訊息\\nAI: 只會記錄這個訊息！'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XQqzWo6K92h"
      },
      "source": [
        "##### 使用 Vector Store 做為儲存後端的記憶單元\n",
        "可以參考 VectorStoreRetrieverMemory 這樣的方法，設計我們的 LLMChain 來擷取背景資料。\n",
        "\n",
        "值得特別提及的是，VectorStoreRetrieverMemory 不只能夠從向量資料庫中檢索相似度資料，它還會在對話過程中將我們的對話記錄保存到向量資料中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAtiXWM1K92h"
      },
      "outputs": [],
      "source": [
        "# 下方是建立向量資料庫的部分\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "\n",
        "db_chroma = Chroma(embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "retriever = db_chroma.as_retriever(search_kwargs=dict(k=1))\n",
        "\n",
        "memory_vs = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
        "\n",
        "# 這裏是模擬我們已經有三個對話記錄\n",
        "memory_vs.save_context({\"Human\": \"我最喜歡的食物是披薩\"}, {\"AI\": \"這樣很棒！\"})\n",
        "memory_vs.save_context({\"Human\": \"我最喜歡的運動是游泳\"}, {\"AI\": \"很高興你跟我說分享你的嗜好。\"})\n",
        "memory_vs.save_context({\"Human\": \"我不喜歡上班\"}, {\"AI\": \"瞭解\"})\n",
        "memory_vs.save_context({\"Human\": \"奇奇自助餐很貴\"}, {\"AI\": \"太糟糕了\"})\n",
        "\n",
        "# 使用 load_memory_varialbes 取得使用者問題相似度的歷史資料\n",
        "print(memory_vs.load_memory_variables({\"prompt\": \"我該看什麼運動節目？\"}))\n",
        "# print(memory_vs.load_memory_variables({\"prompt\": \"昂貴的店家\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VfZwzvK92h"
      },
      "source": [
        "## 整合"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Chain類別](https://python.langchain.com/docs/modules/chains/)"
      ],
      "metadata": {
        "id": "eg7CuDXPKx9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "\n",
        "# 載入資料夾中所有TXT檔案\n",
        "loader = DirectoryLoader('/content/', glob='**/*.txt')\n",
        "\n",
        "# 將資料轉成document物佚，每個檔案會為作為一個document\n",
        "documents = loader.load()\n",
        "\n",
        "# 初始化載入器\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "\n",
        "# 切割加载的 document\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 初始化 openai 的 embeddings 物件\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 將 document 透過 openai 的 embeddings 物件計算 embedding向量資料暫時存入 Chroma 向量資料庫用於後續的搜尋\n",
        "\n",
        "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
        "\n",
        "# 建立檢索器\n",
        "retriever = docsearch.as_retriever()\n",
        "\n",
        "# 建立記憶體\n",
        "memory_vs = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
        "\n",
        "# 設置預設的prompt\n",
        "DEFAULT_TEMPLATE = \"\"\"\n",
        "你是一個友善的對話機器人，下面歷史記錄是我們曾經的對話。\n",
        "Human 是我，AI 是你。請根據歷史記錄中的資訊來回覆我的新問題。\n",
        "\n",
        "歷史記錄:\n",
        "{history}\n",
        "\n",
        "Human：{input}\n",
        "AI：\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=DEFAULT_TEMPLATE\n",
        ")\n",
        "conversation_with_memory_vs = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PROMPT,\n",
        "    memory=memory_vs,\n",
        "    # verbose=True,\n",
        "    output_key='AI'\n",
        ")"
      ],
      "metadata": {
        "id": "e2I11VaMwyOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_memory_vs.predict(input=\"2028總統候選人有誰?\")"
      ],
      "metadata": {
        "id": "oIJ2m1mKy9yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_memory_vs.predict(input=\"我的名字叫做Kevin，很高興認識你\")"
      ],
      "metadata": {
        "id": "pT3fpuNZ2KbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_memory_vs.predict(input=\"你還記得我叫什麼名字嗎?\")"
      ],
      "metadata": {
        "id": "24NKeK6t2m4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}